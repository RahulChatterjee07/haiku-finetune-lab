# ğŸ§ª Haiku Fine-Tune Lab

This project simulates the fine-tuning of AWS Claude Haiku for domain-specific tasks like structured information extraction and Q&A.

---

## ğŸ”§ Features

- ğŸ“¦ Converts training data into Claude-compatible JSONL format
- ğŸ¯ Simulates fine-tuning on custom tasks
- ğŸ“Š Evaluates performance with basic metrics
- ğŸ§  Shows output differences before and after fine-tuning

---

## ğŸ“‚ Project Structure

```
haiku-finetune-lab/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ training_data.jsonl        # Few-shot or synthetic samples
â”‚   â”œâ”€â”€ eval_data.jsonl            # Test data for side-by-side outputs
â”œâ”€â”€ haiku_finetuner/
â”‚   â”œâ”€â”€ prepare_data.py            # Converts raw data into Claude format
â”‚   â”œâ”€â”€ training_config.yaml       # Placeholder configuration
â”‚   â”œâ”€â”€ run_finetune.py            # Simulated Bedrock job submission
â”‚   â””â”€â”€ evaluate_model.py          # Precision/recall/f1 or exact match
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fine_tune_demo.ipynb       # Notebook showing pre vs post fine-tune output
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ’¡ Example Prompt-Completion Training Sample

```json
{
  "prompt": "Extract the total approved amount from the claim: The approved amount is $5,000 CAD.",
  "completion": "$5,000 CAD"
}
```

---

## ğŸ“Š Visual Comparison

### Before vs. After Fine-Tuning

![Before and After Comparison](notebooks/assets/haiku_finetune_comparison.png)

### Token Cost Comparison

![Token Cost Chart](notebooks/assets/haiku_token_cost_comparison.png)

---

## ğŸ§ª Evaluation

- Accuracy of exact match
- JSON formatting compliance
- Answer location consistency

---

Made with â¤ï¸ by Rahul Chatterjee
