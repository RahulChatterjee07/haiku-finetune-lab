{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24476d3e",
   "metadata": {},
   "source": [
    "# ðŸ¤– Claude Haiku Fine-Tune Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load evaluation samples\n",
    "with open(\"../data/eval_data.jsonl\", \"r\") as f:\n",
    "    eval_samples = [json.loads(line) for line in f]\n",
    "\n",
    "# Preview\n",
    "for i, sample in enumerate(eval_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Prompt:\", sample[\"prompt\"])\n",
    "    print(\"Expected:\", sample[\"expected\"])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d635fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate predictions from base vs fine-tuned Haiku\n",
    "def mock_model_predict(prompt, model=\"base\"):\n",
    "    if \"diagnosis\" in prompt.lower():\n",
    "        return \"Post-traumatic stress disorder\" if model == \"finetuned\" else \"stress\"\n",
    "    if \"claim value\" in prompt.lower():\n",
    "        return \"$10,000 CAD\" if model == \"finetuned\" else \"$7,500\"\n",
    "\n",
    "# Compare predictions\n",
    "for i, sample in enumerate(eval_samples):\n",
    "    base_pred = mock_model_predict(sample[\"prompt\"], model=\"base\")\n",
    "    fine_pred = mock_model_predict(sample[\"prompt\"], model=\"finetuned\")\n",
    "    print(f\"Prompt {i+1}: {sample['prompt']}\")\n",
    "    print(\"Expected:\", sample[\"expected\"])\n",
    "    print(\"Base Model:\", base_pred)\n",
    "    print(\"Fine-Tuned:\", fine_pred)\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9550d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "expected = [sample[\"expected\"] for sample in eval_samples]\n",
    "base_preds = [mock_model_predict(s[\"prompt\"], \"base\") for s in eval_samples]\n",
    "fine_preds = [mock_model_predict(s[\"prompt\"], \"finetuned\") for s in eval_samples]\n",
    "\n",
    "base_acc = accuracy_score(expected, base_preds)\n",
    "fine_acc = accuracy_score(expected, fine_preds)\n",
    "\n",
    "print(\"ðŸ“Š Accuracy Comparison:\")\n",
    "print(f\"Base Model Accuracy: {base_acc:.2f}\")\n",
    "print(f\"Fine-Tuned Accuracy: {fine_acc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
